
<html>
	<script>
	  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	  ga('create', 'UA-46217754-1', 'auto');
	  ga('send', 'pageview');

	</script>
	<head>
		<title>
			Licheng Yu - Facebook AI
		</title>
        <link rel="icon" type="image/jpg" href="licheng.jpg" />

		<style>
			body {
				background-color: #eee;
			}
			.main {
				width : 900px;
				margin : 20px auto;
				border-radius: 15px;
				background-color : #fff;
				padding : 50px;
				box-shadow: 0px 0px 10px #999;
                font-family: "Helvetica";
			}
			.intro {
				display: block;
				line-height: 140%;
			}
			a {
				color: #0050e7;
				text-decoration: none;
			}
			a:hover {
				text-decoration : underline;
			}
			a:visited {
				color : #0050e7;
			}
			img.me {
                margin-left: 5px;
				margin-right : 25px;
				border : 0 solid black;
				float : left;
				margin-bottom : 0;
				height: 230px;
			}
			div.projects {
				margin-top: 40px;
			}
            div.news {
                margin-top: 40px;
            }
            div.scroll_news {
                height: 200px;
                width : 900px;
                overflow-y: scroll;
            }
            div.work {
                margin-top: 40px;
            }
			.project-thumb {
				width: 277px;
			}
			.projects-table {
				margin-top: 20px;
			}
			.description-column {
				padding-left: 60px;
				width: 4500px;
			}
			.project-title {
				font-size: 16px;
				font-weight:bold;
				padding-bottom: 5px;
			}
			.description {
				text-align:justify;
				text-justify:inter-word;
				padding-bottom: 5px;
				font-size: 14px;
			}
            .work-column {
                padding-top:5px;
                padding-top:5px;
            }
			.thumb-column {
				padding-top:10px;
				padding-bottom:10px;
			}
            table.pub_table tr {
                outline: thin dotted #666666;
            }
		</style>
	</head>
	<body>
		<div class="main">
			<div class="intro">
				<h1><span class="name">Licheng Yu</span></h1>
				<img class="me" src="images/licheng2019.jpg">
                <p>My name is Licheng Yu (虞立成). I completed my PhD in Computer Science from University of North Carolina at Chapel Hill in 2019 May.
                My advisor is <a target="_blank" href="http://tamaraberg.com">Tamara L. Berg</a>. I also work closely with <a target="_blank" href="http://www.cs.unc.edu/~mbansal/">Mohit Bansal</a> during my PhD study. My research interest lies in computer vision and natural language processing.</p>
                <p> I completed my Master's degrees from both Georgia Tech and Shanghai Jiaotong University in 2014. I received my Bachelor's degree from Shanghai Jiao Tong University. </p>

                <p>
				Email: lichengyu [at] fb.com <br>
                Address: 1 Hacker Way, Menlo Park, CA 94025 <br>
                More info: [<a target="_blank" href="resume.pdf">Resume</a>], [<a target="_blank" href="https://scholar.google.com/citations?user=pwpweRQAAAAJ&hl=en&oi=ao">Google Scholar</a>], [<a target="_blank" href="https://www.linkedin.com/in/licheng-yu-8aa7a8a1/">LinkedIn</a>], [<a target="_blank" href="https://github.com/lichengunc">GitHub</a>].<br>
                </p>
			</div>

            <div class="news">
                <h3>Highlights</h3>
                <div class="scroll_news">
                <ul>
                    <!-- <li><font color="red">Hiring Full-time Research Scientists and 2023 Research Interns who are with AI Generative Modeling (image, video, text) background.</font></li> -->
                    <li>As always: <a target="_blank" href="https://aideadlin.es">Academic Countdown</a></li>
                    <li>2025.03: 4 papers accepted by CVPR 2025.</li>
                    <li>2024.09: Llama3.2 Multimodal 11B+90B released.</li>
                    <li>2024.02: 5 papers accepted by CVPR 2024.</li>
                    <!-- <li>2023.07: 1 paper accepted by ICCV 2023.</li> -->
                    <li>2023.02: 3 papers accepted by CVPR 2023 and 1 paper accepted by ICLR 2023. </li>
                    <li>2022.07: 1 paper accepted by KDD 2022 <a target="_blank" href="https://youtu.be/aMj_h9QHD0o">(Talk)</a> and 2 papers accepted by ECCV 2022. </li>
                    <!-- <li>2021.03: 1 paper accepted by CVPR 2021.</li> -->
                    <!-- <li>2020.09: 2 papers accepted by EMNLP 2020.</li> -->
                    <!-- <li>2020.07: 3 papers accepted by ECCV 2020.</li> -->
                    <li>2020.06: We are <a target="_blank" href="https://visualqa.org/roe.html"><font color="red">Winner of</font> VQA 2020 Challenge</a>.</li>
                    <li>2020.06: Organizer of <a target="_blank" href="https://languageandvision.github.io/">LVVU 2020</a>.
                    <!-- <li>2020.04: 1 paper accepted by ACL 2020.</li> -->
                    <!-- <li>2020.03: 2 papers accepted by CVPR 2020.</li> -->
                    <li>2020.01: <a target="_blank" href="https://tvr.cs.unc.edu/">TVR dataset</a> for Large-Scale Multi-Modal TV Retrieval and Captioning released. </li>
                    <!-- <li>2019.02: 1 paper accepted by CVPR 2019. Perhaps the LAST paper in my PhD study :-) </li> -->
                    <!-- <li>2019.02: 1 paper accepted by NAACL 2019. </li> -->
                    <li>2018.08: Super cool <a target="_blank" href="http://tvqa.cs.unc.edu/">TVQA dataset</a> released.</li>
                    <li>2018.02: <a target="_blank" href="http://vision2.cs.unc.edu/refer/comprehension" style="color:red">Referring Expression Demo</a> online now (in CVPR2018).</li>
                    <!-- <li>Summer intern at Adobe Research. (May, 2017)</li> -->
                    <!-- <li>2017.07: 1 paper accepted by EMNLP 2017.</li> -->
                    <!-- <li>Congrats! UNC won NCAA Championship 2017!</li> -->
                    <li>2017.03: CVPR 2017 <a target="_blank" style="color:red" href="https://www.youtube.com/watch?v=TuamOA-kF-8&list=PL_bDvITUYucBrD_1rwS_fslAsLVudcvua&index=8">(spotlight presentation 8.0%)</a>. Talk is <a target="_blank" href="https://www.youtube.com/watch?v=TuamOA-kF-8&list=PL_bDvITUYucBrD_1rwS_fslAsLVudcvua&index=8">here</a>.</li>
                    <li>2016.07: ECCV 2016 <a target="_blank" style="color:red" href="http://videolectures.net/eccv2016_yu_modeling_context/?q=licheng%20yu">(spotlight presentation 4.7%)</a>. Talk is <a target="_blank" href="http://videolectures.net/eccv2016_yu_modeling_context/?q=licheng%20yu">here</a>. </li>
                    <li>2016.04: RefCOCO and RefCOCO+ dataset released: <a target="_blank" href="https://github.com/lichengunc/refer">Referring Expression Dataset</a></li>
                    <!-- <li>Summer intern at eBay AI team. (May, 2016)</li> -->
                    <!-- <li>2015.10: 1 paper accepted by ICCV 2015. Many thanks to those helping me. </li> -->
                    <li>2015.08: <a target="_blank" href="http://tamaraberg.com/visualmadlibs/">Visual Madlibs dataset</a> released. </li>
                    <!-- <li>2015.01: 1 paper accepted by IEEE Transactions on Image Processing. </li> -->
                    <!-- <li>2014.11: 1 paper accepted by AAAI 2015.  </li> -->
                    <!-- <li>New journey - join UNC <a target="_blank" href="http://bvisionweb1.cs.unc.edu">computer vision group</a>. (Aug, 2014) </li> -->
                </ul>
                </div>
            </div>

            <div class="work">
                <h3>Work Experience</h3>
                <table>
                   <tr>
                        <td class="work-column">
                            <img src="images/facebook_logo2.png" width=165>
                        </td>
                        <td>
                            <p style="font-size: 15px">2023.06&mdash;Present:&nbsp&nbsp&nbsp<br>
                            2022.07&mdash;2023.06:<br>
                            2021.07&mdash;2022.07:<br>
                            2020.03&mdash;2021.07:</p>
                        </td>
                        <td>
                            <p style="font-size: 15px">Research Scientist Manager<br>
                            Staff Research Scientist<br>
                            Senior Research Scientist<br>
                            Research Scientist</p>
                        </td>
                   </tr>
                   <tr>
                        <td class="work-column">
                            <img src="images/msft.png" width=140>
                        </td>
                        <td>
                            <p style="font-size: 15px">2019.06&mdash;2020.03:</p>
                        </td>
                        <td>
                            <p style="font-size: 15px">Researcher</p>
                        </td>
                    </tr>
                    <tr>
                        <td><hr></td>
                        <td>&nbsp &nbsp &nbsp <font color="dimgray">Graduated</font></td>
                        <td><hr></td>
                    </tr>
                    <tr>
                        <td class="work-column">
                            <img src="images/working/unc.gif" width=150>
                        </td>
                        <td>
                            <p style="font-size: 15px">2014.08&mdash;2019.05:</p>
                        </td>
                        <td>
                            <p style="font-size: 15px">Research Assistant</p>
                        </td>
                    </tr>
                    <tr>
                        <td class="work-column" width=190px>
                            <img src="images/facebook_logo2.png" width=165>
                        </td>
                        <td>
                            <p style="font-size: 15px">2018.05&mdash;2018.08:</p>
                        </td>
                        <td>
                            <p style="font-size: 15px">Research Intern</p>
                        </td>
                    </tr>
                    <tr>
                        <td class="work-column">
                            <img src="images/working/adobe.png" width=115>
                        </td>
                        <td>
                            <p style="font-size: 15px">2017.05&mdash;2017.08:</p>
                        </td>
                        <td>
                            <p style="font-size: 15px">Research Intern</p>
                        </td>
                    </tr>
                    <tr>
                        <td class="work-column">
                            <img src="images/working/ebay.png" width=150>
                        </td>
                        <td>
                            <p style="font-size: 15px">2016.05&mdash;2016.08:</p>
                        </td>
                        <td>
                            <p style="font-size: 15px">Research Intern </p>
                        </td>
                    </tr>

                    <tr>
                        <td class="work-column">
                            <img src="images/sjtu.png" width=150>
                        </td>
                        <td>
                            <p style="font-size: 15px">2011.09&mdash;2014.04:<p>
                        </td>
                        <td>
                            <p style="font-size: 15px">Research Assistant <p>
                        </td>
                    </tr>
                </table>
            </div>

			<div class="projects">
				<h3>Projects & Publications</h3>
				<div class="projects-table">
				<table>
                <!-- llama4 -->
                <tr>
                    <td id="reduction" class="thumb-column">
                        <div class="imageContainer">
                            <img class="project-thumb" src="images/llama4.jpg">
                        </div>
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>The Llama 4 herd: The beginning of a new era of natively multimodal AI innovation</b>
                        </div>
                        <div class="description">
                            Llama team
                        </div>
                        <div>
                            [<a target="_blank" href="https://ai.meta.com/blog/llama-4-multimodal-intelligence/">Blog</a>]
                        </div>
                        <div class="description">(Led 17Bx128 and 17Bx16's text+image reinforcement learning Stage)</div>
                    </div>
                </tr>
                <!-- reduction -->
                <tr>
                    <td id="reduction" class="thumb-column">
                        <div class="imageContainer">
                            <img class="project-thumb" src="images/cvpr25_reduction.jpg">
                        </div>
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Accelerating Multimodal Large Language Models by Searching Optimal Vision Token Reduction</b>
                        </div>
                        <a target="_blank" href="https://cvpr.thecvf.com/Conferences/2025">CVPR 2025</a>
                        <div class="description">
                            Shiyu Zhao, Zhenting Wang, Felix Juefei-Xu, Xide Xia, Miao Liu, Xiaofang Wang, Mingfu Liang, Ning Zhang, Dimitris N. Metaxas, <b>Licheng Yu</b><br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/abs/2412.00556">Paper</a>]
                        </div>
                    </div>
                </tr>
                <!-- apollo -->
                <tr>
                    <td id="apollo" class="thumb-column">
                        <div class="imageContainer">
                            <img class="project-thumb" src="images/cvpr25_apollo.jpg">
                        </div>
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Apollo: An Exploration of Video Understanding in Large Multimodal Models</b>
                        </div>
                        <a target="_blank" href="https://cvpr.thecvf.com/Conferences/2025">CVPR 2025</a>
                        <div class="description">
                            Orr Zohar, Xiaohan Wang, Yann Dubois, Nikhil Mehta, Tong Xiao, Philippe Hansen-Estruch, <b>Licheng Yu</b>, Xiaofang Wang, Felix Juefei-Xu, Ning Zhang, Serena Yeung-Levy, Xide Xia<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/abs/2412.10360">Paper</a>]
                        </div>
                    </div>
                </tr>
                <!-- mind -->
                <tr>
                    <td id="mind" class="thumb-column">
                        <div class="imageContainer">
                            <img class="project-thumb" src="images/cvpr25_video.jpg">
                        </div>
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Building a Mind Palace: Structuring Environment-Grounded Semantic Graphs for Effective Long Video Analysis with LLMs</b>
                        </div>
                        <a target="_blank" href="https://cvpr.thecvf.com/Conferences/2025">CVPR 2025</a>
                        <div class="description">
                            Zeiyi Huang, Yuyang Ji, Xiaofang Wang, Nikhil Mehta, Tong Xiao, Donghyun Lee, Sigmund Vanvalkenburgh, Shengxin Zha, Bolin Lai, <b>Licheng Yu</b>, Ning Zhang, Yong Jae Lee, Miao Liu<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/abs/2501.04336">Paper</a>]
                        </div>
                    </div>
                </tr>
                <!-- roictrl -->
                <tr>
                    <td id="roictrl" class="thumb-column">
                        <div class="imageContainer">
                            <img class="project-thumb" src="images/cvpr25_roictrl.jpg">
                        </div>
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>ROICtrl: Boosting Instance Control for Visual Generation</b>
                        </div>
                        <a target="_blank" href="https://cvpr.thecvf.com/Conferences/2025">CVPR 2025</a>
                        <div class="description">
                            Yuchao Gu, Yipin Zhou, Yunfan Ye, Yinxin Nie, <b>Licheng Yu</b>, Pingchuan Ma, Kevin Qinghong Lin, Mike Zheng Shou<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/abs/2411.17949">Paper</a>][<a target="_blank" href="https://roictrl.github.io/">Project</a>]
                        </div>
                    </div>
                </tr>
                <!-- Llama3.2 -->
                <tr>
                    <td id="avid" class="thumb-column">
                        <div class="imageContainer">
                            <img class="project-thumb" src="images/llama3.2.jpg">
                        </div>
                    </td>
                    <td class="description-column">
                        <div>
                            <div class="project-title">
                                <b>The Llama 3 Herd of Models</b>
                            </div>
                            <a target="_blank" href="https://arxiv.org/pdf/2407.21783">arXiv:2407.21783v2</a>
                            <div class="description">
                                Llama team
                            </div>
                            <div>
                                [<a target="_blank" href="https://arxiv.org/pdf/2407.21783">Paper</a>][<a target="_blank" href="https://www.llama.com/">Project</a>][<a target="_blank" href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/">Blog</a>]
                            </div>
                            <div class="description">(Led Llama3.2 Multimodal 11B/90B Pre-training + 11B Post-training)</div>
                        </div>
                    </td>
                </tr>
               <!-- animated_sticker -->
               <tr>
                <td id="avid" class="thumb-column">
                    <div class="imageContainer">
                        <img class="project-thumb" src="images/animated_sticker.jpg">
                    </div>
                </td>
                <td class="description-column">
                <div>
                    <div class="project-title">
                        <b>Animated Stickers: Bringing Stickers to Life with Video Diffusion</b>
                    </div>
                    <a target="_blank" href="https://arxiv.org/abs/2402.06088">arXiv:2402.06088</a>
                    <div class="description">
                        David Yan, Winnie Zhang, Luxin Zhang, Anmol Kalia, Dingkang Wang, Ankit Ramchandani, Miao Liu, Albert Pumarola, Edgar Schoenfeld, Elliot Blanchard, Krishna Narni, Yaqiao Luo, Lawrence Chen, Guan Pang, Ali Thabet, Peter Vajda, Amy Bearman, <b>Licheng Yu</b><br>
                    </div>
                    <div>
                        [<a target="_blank" href="https://arxiv.org/abs/2402.06088">Paper</a>]
                    </div>
                </div>
            </tr>
                <!-- avid -->
                <tr>
                    <td id="avid" class="thumb-column">
                        <div class="imageContainer">
                            <img class="project-thumb" src="images/avid.jpeg">
                        </div>
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>AVID: Any-Length Video Inpainting with Diffusion Model</b>
                        </div>
                        <a target="_blank" href="https://cvpr.thecvf.com/Conferences/2024">CVPR 2024</a>
                        <div class="description">
                            Zhixing Zhang, Bichen Wu, Xiaoyan Wang, Yaqiao Luo, Luxin Zhang, Yinan Zhao, Peter Vajda, Dimitris Metaxas, <b>Licheng Yu</b><br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/abs/2312.03816">Paper</a>][<a target="_blank" href="https://zhang-zx.github.io/AVID/">Project</a>][<a target="_blank" href="https://youtu.be/p8-xTfOe9Tw?si=2lzt0o52XPenprPx">Video</a>]
                        </div>
                    </div>
                </tr>
                <!-- scene_text -->
                <tr>
                    <td id="avid" class="thumb-column">
                        <div class="imageContainer">
                            <img class="project-thumb" src="images/cvpr24_scenetext.jpg">
                        </div>
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>SceneTextGen: Layout-Agnostic Scene Text Image Synthesis with Integrated Character-Level Diffusion and Contextual Consistency</b>
                        </div>
                        <a target="_blank" href="https://cvpr.thecvf.com/Conferences/2024">CVPR 2024</a>
                        <div class="description">
                             Qilong Zhangli, Praveen Krishnan, Ankit Ramchandani, Xiaoliang Dai, <b>Licheng Yu</b>, Di Liu, Jindong Jiang, Dimitris N. Metaxas, Guan Pang<br>
                        </div>
                        <div>
                            [<a target="_blank" href="">Paper</a>]
                        </div>
                    </div>
                </tr>
                <!-- flowvid -->
                <tr>
                    <td id="flowid" class="thumb-column">
                        <div class="imageContainer">
                            <img class="project-thumb" src="images/flowvid.jpg">
                        </div>
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>FlowVid: Taming Imperfect Optical Flows for Consistent Video-to-Video Synthesis</b>
                        </div>
                        <a target="_blank" href="https://cvpr.thecvf.com/Conferences/2024">CVPR 2024</a>
                        <div class="description">
                            Feng Liang, Bichen Wu, Jialiang Wang, <b>Licheng Yu</b>, Kunpeng Li, Yinan Zhao, Ishan Misra, Jia-Bin Huang, Peizhao Zhang, Peter Vajda, Diana Marculescu<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/abs/2312.17681">Paper</a>][<a target="_blank" href="https://jeff-liangf.github.io/projects/flowvid/">Project</a>]
                        </div>
                    </div>
                </tr>
                <!-- fairy -->
                <tr>
                    <td id="fairy" class="thumb-column">
                        <div class="imageContainer">
                            <img class="project-thumb" src="images/fairy_method.jpg">
                        </div>
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Fairy: Fast Parallelized Instruction-Guided Video-to-Video Synthesis</b>
                        </div>
                        <a target="_blank" href="https://cvpr.thecvf.com/Conferences/2024">CVPR 2024</a>
                        <div class="description">
                            Bichen Wu, Ching-Yao Chuang, Xiaoyan Wang, Yichen Jia, Kapil Krishnakumar, Tong Xiao, Feng Liang, <b>Licheng Yu</b>, Peter Vajda<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/abs/2312.13834">Paper</a>][<a target="_blank" href="https://fairy-video2video.github.io/">Project</a>]
                        </div>
                    </div>
                </tr>
                <!-- videoswap -->
                <tr>
                    <td id="videoswap" class="thumb-column">
                        <div class="imageContainer">
                            <img class="project-thumb" src="images/videoswap.jpg">
                        </div>
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>VideoSwap: Customized Video Subject Swapping with
Interactive Semantic Point Correspondence</b>
                        </div>
                        <a target="_blank" href="https://cvpr.thecvf.com/Conferences/2024">CVPR 2024</a>
                        <div class="description">
                            Yuchao Gu, Yipin Zhou, Bichen Wu, <b>Licheng Yu</b>, Jia-Wei Liu, Rui Zhao, Jay Zhangjie Wu, David Junhao Zhang, Mike Zheng Shou, Kevin Tang<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/abs/2312.02087">Paper</a>][<a target="_blank" href="https://videoswap.github.io/">Project</a>]
                        </div>
                    </div>
                </tr>
                <!-- sticker -->
                <tr>
                    <td id="sticker" class="thumb-column">
                        <div class="imageContainer">
                            <img class="project-thumb" src="images/sticker.png">
                        </div>
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Text-to-Sticker: Style Tailoring Latent Diffusion Models for Human Expression</b>
                        </div>
                        <a target="_blank" href="https://arxiv.org/abs/2311.10794">arXiv:2311.10794</a>
                        <div class="description">
                            Animesh Sinha, Bo Sun, Anmol Kalia, Arantxa Casanova, Elliot Blanchard, David Yan, Winnie Zhang, Tony Nelli, Jiahui Chen, Hardik Shah, <b>Licheng Yu</b>, Mitesh Kumar Singh, Ankit Ramchandani, Maziar Sanjabi, Sonal Gupta, Amy Bearman, Dhruv Mahajan<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/abs/2311.10794">Paper</a>]
                        </div>
                    </div>
                </tr>
                <!-- CiT -->
                <tr>
                    <td id="cit" class="thumb-column">
                        <div class="imageContainer">
                            <img class="project-thumb" src="images/iccv23_cit.jpg">
                        </div>
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>CiT: Curation in Training for Effective Vision-Language Data</b>
                        </div>
                        <a target="_blank" href="https://iccv2023.thecvf.com/">ICCV 2023</a>
                        <div class="description">
                            Hu Xu, Saining Xie, Po-Yao Huang, <b>Licheng Yu</b>, Russell Howes, Gargi Ghosh
Luke Zettlemoyer, Christoph Feichtenhofe<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/abs/2301.02241">Paper</a>][<a target="_blank" href="https://github.com/facebookresearch/CiT">Code</a>]
                        </div>
                    </div>
                </tr>
                <!-- TVC -->
                <tr>
                    <td id="tvc" class="thumb-column">
                        <div class="imageContainer">
                            <img class="project-thumb" src="images/tvc.jpg">
                        </div>
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Tell Me What Happened: Unifying Text-guided Video Completion via Multimodal Masked Video Generation</b>
                        </div>
                        <a target="_blank" href="https://cvpr2023.thecvf.com/">CVPR 2023</a>
                        <div class="description">
                            Tsu-Jui Fu, <b>Licheng Yu</b>, Ning Zhang, Cheng-Yang Fu, Jong-Chyi Su, William Yang Wang, Sean Bell<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/abs/2211.12824">Paper</a>][<a target="_blank" href="https://tvc-mmvg.github.io/">Project</a>][<a target="_blank" href="https://github.com/tsujuifu/pytorch_tvc">Code</a>][<a target="_blank" href="https://tsujuifu.github.io/slides/cvpr23_tvc.ppsx">Slides</a>][<a target="_blank" href="https://www.youtube.com/watch?v=dnBzUfsf9Cc&feature=youtu.be">Video</a>]
                        </div>
                    </div>
                </tr>
                <!-- Procedure -->
                <tr>
                    <td id="procedure" class="thumb-column">
                        <div class="imageContainer">
                            <img class="project-thumb" src="images/cvpr23_procedure.jpg">
                        </div>
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Learning Procedure-aware Video Representation from Instructional Videos and Their Narrations</b>
                        </div>
                        <a target="_blank" href="https://cvpr2023.thecvf.com/">CVPR 2023</a>
                        <div class="description">
                            Yiwu Zhong, <b>Licheng Yu</b>, Yang Bai, Shangwen Li, Xueting Yan, Yin Li<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/abs/2303.17839">Paper</a>][<a target="_blank" href="https://github.com/facebookresearch/ProcedureVRL">Code</a>]
                        </div>
                    </div>
                </tr>
                <!-- FAME-ViL -->
                <tr>
                    <td id="faim" class="thumb-column">
                        <div class="imageContainer">
                            <img class="project-thumb" src="images/cvpr23_fame.jpg">
                        </div>
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>FAME-ViL: Multi-Tasking Vision-Language Model for Heterogeneous Fashion Tasks</b>
                        </div>
                        <a target="_blank" href="https://cvpr2023.thecvf.com/">CVPR 2023</a>
                        <div class="description">
                            Xiao Han, Xiatian Zhu, <b>Licheng Yu</b>, Li Zhang, Yi-Zhe Song, Tao Xiang<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/abs/2303.02483">Paper</a>][<a target="_blank" href="https://github.com/BrandonHanx/FAME-ViL">Code</a>] <font color="red">(Oral)</font>
                        </div>
                    </div>
                </tr>
                <!-- instruction -->
                <tr>
                    <td id="instruction" class="thumb-column">
                        <div class="imageContainer">
                            <img class="project-thumb" src="images/inst_video.jpg">
                        </div>
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Learning and Verification of Task Structure in Instructional Videos</b>
                        </div>
                        <a target="_blank" href="https://arxiv.org/abs/2303.13519/">arXiv:2303.13519</a>
                        <div class="description">
                            Medhini Narasimhan, <b>Licheng Yu</b>, Sean Bell, Ning Zhang, Trevor Darrell<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/abs/2303.13519">Paper</a>][<a target="_blank" href="https://medhini.github.io/task_structure/">Project</a>]
                        </div>
                    </div>
                </tr>
                <!-- AMELI -->
                <tr>
                    <td id="ameli" class="thumb-column">
                        <div class="imageContainer">
                            <img class="project-thumb" src="images/ameli.jpg">
                        </div>
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>AMELI: Enhancing Multimodal Entity Linking with Fine-Grained Attributes</b>
                        </div>
                        <a target="_blank" href="https://arxiv.org/abs/2305.14725">arXiv:2305.14725</a>
                        <div class="description">
                            Barry Menglong Yao, Yu Chen, Qifan Wang, Sijia Wang, Minqian Liu, Zhiyang Xu, <b>Licheng Yu</b>, Lifu Huang<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/abs/2305.14725">Paper</a>]
                        </div>
                    </div>
                </tr>
                <!-- ropaws -->
                <tr>
                    <td id="ropaws" class="thumb-column">
                        <div class="imageContainer">
                            <img class="project-thumb" src="images/iclr23_paws.jpg">
                        </div>
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>RoPAWS: Robust Semi-supervised Representation Learning from Uncurated Data</b>
                        </div>
                        <a target="_blank" href="https://iclr.cc/">ICLR 2023</a>
                        <div class="description">
                            Sangwoo Mo, Jong-Chyi Su, Kevin Chih-Yao Ma, Mido Assran, Ishan Misra, <b>Licheng Yu</b>, Sean Bell<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/abs/2302.14483">Paper</a>]
                        </div>
                    </div>
                </tr>
                <!-- Que2Engage-->
                <tr>
                    <td id="que2engage" class="thumb-column">
                        <div class="imageContainer">
                            <img class="project-thumb" src="images/www23.png">
                        </div>
                    </td>
                    <td class="description-column">
                    <div>
                        <br>
                        <div class="project-title">
                            <b>Que2Engage: Embedding-based Retrieval for Relevant and Engaging Products at Facebook Marketplace</b>
                        </div>
                        <a target="_blank" href="https://www2023.thewebconf.org/">WWW 2023</a>
                        <div class="description">
                            Yunzhong He, Yuxin Tian, Mengjiao Wang, Feier Chen, <b>Licheng Yu</b>, Maolong Tang, Congcong Chen, Ning Zhang, Bin Kuang, Arul Prakash<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/abs/2302.11052">Paper</a>]
                        </div>
                    </div>
                </tr>
                <!-- Fad-VLP -->
                <tr>
                    <td id="fad" class="thumb-column">
                        <div class="imageContainer">
                            <img class="project-thumb" src="images/emnlp2022_fad.jpg">
                        </div>
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>FaD-VLP: Fashion Vision-and-Language Pre-training towards Unified Retrieval and Captioning</b>
                        </div>
                        <a target="_blank" href="https://2022.emnlp.org/">EMNLP 2022</a>
                        <div class="description">
                            Suvir Mirchandani, <b>Licheng Yu</b>, Mengjiao Wang, Animesh Sinha, Wenwen Jiang, Tao Xiang, Ning Zhang<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/abs/2210.15028">Paper</a>]
                        </div>
                    </div>
                </tr>
                <!-- FashionViL -->
                <tr>
                    <td id="fashionvil" class="thumb-column">
                        <img class="project-thumb" src="images/eccv2022_fashionvil2.jpg">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>FashionViL: Fashion-Focused Vision-and-Language Representation Learning</b>
                        </div>
                        <a target="_blank" href="https://eccv2022.ecva.net/">ECCV 2022</a>
                        <div class="description">
                            Xiao Han, <b>Licheng Yu</b>, Xiatian Zhu, Li Zhang, Yi-Zhe Song, Tao Xiang<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/abs/2207.08150">Paper</a>][<a target="_blank" href="https://github.com/BrandonHanx/mmf">Code</a>]
                        </div>
                    </div>
                </tr>

                <!-- GEBC -->
                <tr>
                    <td id="gebc" class="thumb-column">
                        <img class="project-thumb" src="images/gebc.jpg">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Generic Event Boundary Captioning: A Benchmark for Status Changes Understanding</b>
                        </div>
                        <a target="_blank" href="https://eccv2022.ecva.net/">ECCV 2022</a>
                        <div class="description">
                            Yuxuan Wang, Difei Gao, <b>Licheng Yu</b>, Weixian Lei, Matt Feiszli, Mike Zheng Shou<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/abs/2204.00486">Paper</a>]
                        </div>
                    </div>
                </tr>
                <!-- CommerceMM -->
                <tr>
                    <td id="commercemm" class="thumb-column">
                        <img class="project-thumb" src="images/kdd22_commercemm.png">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>CommerceMM: Large-Scale Commerce MultiModal Representation Learning with Omni Retrieval</b>
                        </div>
                        <a target="_blank" href="https://kdd.org/kdd2022/">KDD 2022</a>
                        <div class="description">
                            <b>Licheng Yu</b>, Jun Chen, Animesh Sinha, Mengjiao Wang, Yu Chen, Tamara L. Berg, Ning Zhang<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/abs/2202.07247">Paper</a>][<a target="_blank" href="https://ai.facebook.com/blog/commercemm-a-new-approach-to-multimodal-understanding-for-online-shopping/">Blog</a>][<a target="_blank" href="images/kdd2022_commercemm_poster.pdf">Poster</a>][<a target="_blank" href="https://youtu.be/aMj_h9QHD0o">Talk</a>]
                        </div>
                    </div>
                </tr>
                <!-- UVLP -->
                <tr>
                    <td id="uvlp" class="thumb-column">
                        <img class="project-thumb" src="images/uvlp.png">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Unsupervised Vision-and-Language Pre-training via Retrieval-based
Multi-Granular Alignment</b>
                        </div>
                        <a target="_blank" href="https://cvpr2022.thecvf.com/">CVPR 2022</a>
                        <div class="description">
                            Mingyang Zhou*, <b>Licheng Yu</b>*, Amanpreet Singh, Mengjiao Wang, Yu Zhou, Ning Zhang<br>(*First 2 authors contribute equally.)
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/abs/2203.00242">Paper</a>][<a target="_blank" href="https://github.com/zmykevin/UVLP">Code</a>] <font color="red">(Oral)</font>
                        </div>
                </tr>
                <!-- LoopITR -->
                <tr>
                    <td id="uvlp" class="thumb-column">
                        <img class="project-thumb" src="images/loopitr.jpg">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>LOOPITR: Combining Dual and Cross Encoder Architectures for Image-Text Retrieval</b>
                        </div>
                        <a target="_blank" href="https://arxiv.org/pdf/2203.05465.pdf">arxiv:2203.05465v1</a>
                        <div class="description">
                            Jie Lei, Xinlei Chen, Ning Zhang, Mengjiao Wang, Mohit Bansal, Tamara L. Berg, <b>Licheng Yu</b>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/pdf/2203.05465.pdf">Paper</a>]
                        </div>
                </tr>

                <!-- value -->
                <tr>
                    <td id="value" class="thumb-column">
                        <img class="project-thumb" src="images/value2.jpg">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation</b>
                        </div>
                            <a target="_blank" href="https://nips.cc/Conferences/2021">NeurIPS 2021</a>
                        <div class="description">
                            Linjie Li, Jie Lei, Zhe Gan, <b>Licheng Yu</b>, Yen-Chun Chen, Rohit Pillai, Yu Cheng, Luowei Zhou, Xin Eric Wang, William Yang Wang, Tamara L. Berg, Mohit Bansal, Jingjing Liu, Lijuan Wang, Zicheng Liu<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/pdf/2106.04632.pdf">Paper</a>][<a target="_blank" href="https://value-leaderboard.github.io/"><font color="red">Leaderboard</font></a>]
                        </div>
                    </div>
                </tr>
                <!-- trace -->
                <tr>
                    <td id="hero" class="thumb-column">
                        <img class="project-thumb" src="images/cvpr2021_trace.jpg">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Connecting What to Say With Where to Look by Modeling Human Attention Traces</b>
                        </div>
                        <a target="_blank" href="http://cvpr2021.thecvf.com/">CVPR 2021</a>
                        <div class="description">
                            Zihang Meng, <b>Licheng Yu</b>, Ning Zhang, Tamara L. Berg, Babak Damavandi, Vikas Singh, Amy Bearman<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/abs/2105.05964">Paper</a>][<a target="_blank" href="https://github.com/facebookresearch/connect-caption-and-trace">Code</a>]
                        </div>
                </tr>
                <!-- vlep -->
                <tr>
                    <td id="hero" class="thumb-column">
                        <img class="project-thumb" src="images/emnlp20_vlep.jpg">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>What is More Likely to Happen Next? Video-and-Language Future Event Prediction</b>
                        </div>
                        <a target="_blank" href="https://2020.emnlp.org/">EMNLP 2020</a>
                        <div class="description">
                            Jie Lei, <b>Licheng Yu</b>, Tamara L. Berg, Mohit Bansal<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/pdf/2010.07999.pdf">Paper</a>][<a target="_blank" href="https://github.com/jayleicn/VideoLanguageFuturePred">Code</a>]
                        </div>
                </tr>
                <!-- hero -->
                <tr>
                    <td id="hero" class="thumb-column">
                        <img class="project-thumb" src="images/hero.jpg">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>HERO: Hierarchical Encoder for Video+Language Omni-representation Pre-training</b>
                        </div>
                        <a target="_blank" href="https://2020.emnlp.org/">EMNLP 2020</a>
                        <div class="description">
                            Linjie Li*, Yen-Chun Chen*, Yu Cheng, Zhe Gan, <b>Licheng Yu</b>, Jingjing Liu<br>
                            (*First 2 authors contribute equally.)
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/pdf/2005.00200.pdf">Paper</a>][<a target="_blank" href="https://github.com/linjieli222/HERO">Code</a>][<a target="_blank" href="https://towardsdatascience.com/hero-youll-never-have-to-watch-long-videos-again-ee0d5e2ba4fd">Blog</a>]
                        </div>
                        <a target="_blank" href="https://tvr.cs.unc.edu/index.html"><font color="red">Rank 1 on</font> TVR Leaderboard</a><br>
                        <a target="_blank" href="https://tvr.cs.unc.edu/tvc.html"><font color="red">Rank 1 on</font> TVC Leaderboard</a>
                    </div>
                </tr>
                <!-- value -->
                <tr>
                    <td id="value" class="thumb-column">
                        <img class="project-thumb" src="images/value.png">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Behind the Scene: Revealing the Secrets of
Pre-trained Vision-and-Language Models</b>
                        </div>
                            <a target="_blank" href="https://eccv2020.eu/">ECCV 2020</a>
                        <div class="description">
                            Jize Cao, Zhe Gan, Yu Cheng, <b>Licheng Yu</b>, Yen-Chun Chen, Jingjing Liu<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/pdf/2005.07310.pdf">Paper</a>] <font color="red">(Spotlight)</font>
                        </div>
                    </div>
                </tr>
                <!-- TVR/TVC -->
                <tr>
                    <td id="tvqa" class="thumb-column">
                        <img class="project-thumb" src="images/tvr_example.jpg">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>TVR: A Large-Scale Dataset for Video-Subtitle Moment Retrieval</b>
                        </div>
                            <a target="_blank" href="https://eccv2020.eu/">ECCV 2020</a>
                        <div class="description">
                            Jie Lei, <b>Licheng Yu</b>, Tamara L. Berg, Mohit Bansal<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/pdf/2001.09099.pdf">Paper</a>]
                            [<a target="_blank" href="https://tvr.cs.unc.edu">Dataset</a>]
                            [<a target="_blank" href="https://github.com/jayleicn/TVRetrieval">Code (Retrieval)</a>]
                            [<a target="_blank" href="https://github.com/jayleicn/TVCaption">Code (Captioning)</a>]
                        </div>
                    </div>
                </tr>
                <!-- UNITER -->
                <tr>
                    <td id="tvqa" class="thumb-column">
                        <img class="project-thumb" src="images/uniter.jpg">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>UNITER: Learning UNiversal Image-Text Representations</b>
                        </div>
                            <a target="_blank" href="https://eccv2020.eu/">ECCV 2020</a>
                        <div class="description">
                            Yen-Chun Chen*, Linjie Li*, <b>Licheng Yu</b>*, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, Jingjing Liu <br> (*First 3 authors contribute equally.)
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/pdf/1909.11740.pdf">Paper</a>]
                            [<a target="_blank" href="https://github.com/lichengunc/pretrain-vl-data">Dataset</a>]
                            [<a target="_blank" href="https://github.com/ChenRocks/UNITER">Code</a>]
                        </div>
                        <div>
                            Achieving SOTA on 13 Vision+Language Datasets/Tasks, and
                            <br>
                            <a target="_blank" href="https://visualcommonsense.com/leaderboard/"><font color="red">Rank 1 on</font> VCR Leaderboard</a>
                            <br>
                            <a target="_blank" href="http://lil.nlp.cornell.edu/nlvr/"><font color="red">Rank 1 on</font> NLVR2 Leaderboard</a>
                        </div>
                    </div>
                </tr>
                <!-- TVQA+ -->
                <tr>
                    <td id="tvqa" class="thumb-column">
                        <img class="project-thumb" src="images/tvqa+.jpg">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>TVQA+: Spatio-Temporal Grounding for Video Question Answering</b>
                        </div>
                            <a target="_blank" href="https://acl2020.org/">ACL 2020</a>
                        <div class="description">
                            Jie Lei, <b>Licheng Yu</b>, Tamara L. Berg, Mohit Bansal<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/pdf/1904.11574.pdf">Paper</a>]
                            [<a target="_blank" href="http://tvqa.cs.unc.edu/download_tvqa_plus.html">Dataset</a>]
                        </div>
                    </div>
                </tr>
                <!-- BachGan -->
                <tr>
                    <td id="bachgan" class="thumb-column">
                        <img class="project-thumb" src="images/bachgan.png">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>BachGAN: High-Resolution Image Synthesis from Salient Object Layout</b>
                        </div>
                            <a target="_blank" href="http://cvpr2020.thecvf.com/">CVPR 2020</a>
                        <div class="description">
                            Yandong Li, Yu Cheng, Zhe Gan, <b>Licheng Yu</b>, Liqiang Wang, Jingjing Liu<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/pdf/2003.11690.pdf">Paper</a>][<a target="_blank" href="https://github.com/Cold-Winter/BachGAN">Code</a>]
                        </div>
                    </div>
                </tr>
                <!-- Violin -->
                <tr>
                    <td id="bachgan" class="thumb-column">
                        <img class="project-thumb" src="images/cvpr2020_violin.png">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>VIOLIN: A Large-Scale Dataset for Video-and-Language Inference</b>
                        </div>
                            <a target="_blank" href="http://cvpr2020.thecvf.com/">CVPR 2020</a>
                        <div class="description">
                            Jingzhou Liu, Wenhu Chen, Yu Cheng, Zhe Gan, <b>Licheng Yu</b>, Yiming Yang, Jingjing Liu<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/pdf/2003.11618v1.pdf">Paper</a>]
                            [<a target="_blank" href="https://github.com/jimmy646/violin">Dataset</a>]
                        </div>
                    </div>
                </tr>
                <!-- MT-EQA -->
                <tr>
                    <td id="mt-eqa" class="thumb-column">
                        <img class="project-thumb" src="images/cvpr19_mteqa.jpg">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Multi-Target Embodied Question Answering</b>
                        </div>
                        <a target="_blank" href="http://cvpr2019.thecvf.com/">CVPR 2019</a>
                        <div class="description">
                            <b>Licheng Yu</b>, Xinlei Chen, Georgia Gkioxari, Mohit Bansal, Tamara L. Berg, Dhruv Batra<br>
                        </div>
                        <div>
                            [<a target="_blank" href='https://arxiv.org/abs/1904.04686'>Paper</a>]
                            [<a target="_blank" href="https://youtu.be/pK5gYk9OgjE">Video</a>]
                        </div>
                        <div>
                        </div>
                    </div>
                </tr>
                <!-- instr-nav -->
                <tr>
                    <td id="mt-eqa" class="thumb-column">
                        <img class="project-thumb" src="images/naacl19_intro.jpg">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Learning to Navigate Unseen Environments: Back Translation with Environmental Dropout</b>
                        </div>
                        <a target="_blank" href="https://naacl2019.org/">NAACL 2019</a>
                        <div class="description">
                            Hao Tan, <b>Licheng Yu</b>, Mohit Bansal<br>
                        </div>
                       <div>
                            <a target="_blank" href="https://evalai.cloudcv.org/web/challenges/challenge-page/97/overview">Room-to-Room Vision-Language-Navigation Leaderboard <font color="red">Rank 1</font></a>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/pdf/1904.04195.pdf">Paper</a>] [<a target="_blank" href="https://github.com/airsplay/R2R-EnvDrop">Code</a>]
                        </div>
                        <div>
                        </div>
                    </div>
                </tr>
                <!-- TVQA -->
                <tr>
                    <td id="tvqa" class="thumb-column">
                        <img class="project-thumb" src="images/tvqa.jpg">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>TVQA: Localized Compositional Video Question Answering</b>
                        </div>
                            <a target="_blank" href="http://emnlp2018.org/">EMNLP 2018</a>
                        <div class="description">
                            Jie Lei, <b>Licheng Yu</b>, Mohit Bansal, Tamara L. Berg<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/abs/1809.01696">Paper</a>]
                            [<a target="_blank" href="http://tvqa.cs.unc.edu/">Project</a>]
                            [<a target="_blank" href="http://tvqa.cs.unc.edu/explore.html">Explore</a>]
                            <font color="red">(Oral)</font>
                        </div>
                    </div>
                </tr>
                <!-- MAttNet -->
                <tr>
                    <td id="MAttNet" class="thumb-column">
                        <img class="project-thumb" src="images/matnet.jpeg">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>MAttNet: Modular Attention Network for Referring Expression Comprehension</b>
                        </div>
                        <a target="_blank" href="http://cvpr2018.thecvf.com/">CVPR 2018</a>
                        <div class="description">
                            <b>Licheng Yu</b>, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, Tamara L. Berg<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/pdf/1801.08186.pdf">Paper</a>]
                            [<a target="_blank" href="http://vision2.cs.unc.edu/refer/comprehension"><font color="red">Demo</font></a>]
                            [<a target="_blank" href="https://github.com/lichengunc/MAttNet">Code</a>]
                            [<a target="_blank" href="http://vision2.cs.unc.edu/refer/">Project</a>]
                        </div>
                    </div>
                    </td>
                </tr>
                <!-- JNLE -->
                <tr>
                    <td id="JNLE" class="thumb-column">
                        <img class="project-thumb" src="images/jnle.png">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>From Image to Language and Back Again</b>
                        </div>
                        <a target="_blank" href="https://www.cambridge.org/core/journals/natural-language-engineering">Journal of Natural Language Engineering</a> (JNLE), 2018
                        <div class="description">
                            Anya Belz, Tamara L. Berg, <b>Licheng Yu</b>
                        </div>
                        <div>
                            [<a target="_blank" href="papers/nle2018.pdf">Paper</a>]
                        </div>
                    </div>
                    </td>
                </tr>

                <!-- Garment Recovery -->
                <tr>
                    <td id="Garment Recovery" class="thumb-column">
                        <img class="project-thumb" src="images/tog.jpg">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Physics-Inspired Garment Recovery from a Single-View Image</b>
                        </div>
                        <a target="_blank" href="https://tog.acm.org/">ACM Transactions on Graphics</a>, 2018
                        <div class="description">
                            Shan Yang, Tanya Ambert, Zherong Pan, Ke Wang, <b>Licheng Yu</b>, Tamara L. Berg, Ming C. Lin<br>
                        </div>
                        <div>
                            [<a target="_blank" href="http://arxiv.org/abs/1608.01250">Paper</a>][<a target="_blank" href="http://www.cs.unc.edu/~alexyang/videos/SIGASIA2016_demo_v2_540_1mq.mp4">Video</a>]
                        </div>
                    </div>
                    </td>
                </tr>

                <!-- Manifold Learning -->
                <tr>
                    <td id="Manifold Learning" class="thumb-column">
                        <img class="project-thumb" src="images/manifold2.png">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>A Unified Framework for Manifold Landmarking</b>
                        </div>
                        <a target="_blank" href="https://ieeexplore.ieee.org/document/8456613">IEEE Transactions on Signal Processing</a>, 2018
                        <div class="description">
                            Hongteng Xu, <b>Licheng Yu</b>, Mark Davenport, Hongyuan Zha<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/pdf/1710.09334.pdf">Paper</a>]
                        </div>
                    </div>
                </tr>

                <!-- Album Summarization -->
                <tr>
                    <td id="Album summarization" class="thumb-column">
                        <img class="project-thumb" src="images/emnlp17_model.jpeg">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Hierarchically-Attentive RNN for Album Summarization and Storytelling</b>
                        </div>
                            <a target="_blank" href="http://emnlp2017.net/">EMNLP 2017</a>
                        <div class="description">
                            <b>Licheng Yu</b>, Mohit Bansal, Tamara L. Berg<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/abs/1708.02977">Paper</a>]
                            [<a target="_blank" href="https://github.com/lichengunc/vist_api">Dataset API</a>]
                        </div>
                    </div>
                </tr>
                <!-- Speaker and Listener -->
                <tr>
                    <td id="Speaker and Listener" class="thumb-column">
                        <img class="project-thumb" src="images/arxiv17_model.jpg">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>A Joint Speaker-Listener-Reinforcer Model for Referring Expressions</b>
                        </div>
                        <a target="_blank" href="http://cvpr2017.thecvf.com/">CVPR 2017</a>
                        <div class="description">
                            <b>Licheng Yu</b>, Hao Tan, Mohit Bansal, Tamara L. Berg<br>
                        </div>
                        <div>
                            [<a target="_blank" href="https://arxiv.org/abs/1612.09542">Paper</a>]
                            [<a target="_blank" href="https://github.com/lichengunc/speaker_listener_reinforcer">Code</a>]
                            [<a target="_blank" href="http://bvisionweb1.cs.unc.edu/refer/">Project</a>]
                            [<font color="red"><a target="_blank" href="https://www.youtube.com/watch?v=TuamOA-kF-8&list=PL_bDvITUYucBrD_1rwS_fslAsLVudcvua&index=8">Talk</a></font>]
                            <!-- [<a target="_blank" href="https://github.com/lichengunc/visdif_emb_guide2_reinforce">Code</a>] -->
                            <font color="red">(Spotlight presentation 8%)</font>
                        </div>

                    </div>
                </tr>

                <!-- Refer-it expression -->
                <tr>
                    <td id="Refer Expression" class="thumb-column">
                        <img class="project-thumb" src="images/eccv16.jpg">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Modeling Context in Referring Expressions</b>
                        </div>
                        <a target="_blank" href="http://www.eccv2016.org/">ECCV 2016</a><br>
                        <div class="description">
                            <b>Licheng Yu</b>, Patrick Poirson, Shan Yang, Alexander C. Berg, Tamara L. Berg<br>
                        </div>
                        <div>
                            [<a target="_blank" href="http://arxiv.org/abs/1608.00272">Paper</a>]
                            [<a target="_blank" href="https://github.com/lichengunc/refer">Dataset</a>]
<!--                             [<a target="_blank" href="http://videolectures.net/eccv2016_yu_modeling_context/?q=licheng%20yu">Talk</a>][<a target="_blank" href="http://referitgame.com/">Online Game</a>] -->
                            [<a target="_blank" href="http://videolectures.net/eccv2016_yu_modeling_context/?q=licheng%20yu">Talk</a>]
                            <font color="red">(Spotlight presentation 4.7%)</font>
                      </div>
                    </div>
                    </td>
                </tr>

                <!-- Visual Madlibs -->
                <tr>
                    <td id="Visual Madlibs" class="thumb-column">
                        <img class="project-thumb" src="images/iccv15_madlibs2.png">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Visual Madlibs: Fill-in-the-blank Image Description and Question Answering</b>
                        </div>
                        <a target="_blank" href="http://pamitc.org/iccv15/">ICCV 2015</a>
                        <div class="description">
                            <b>Licheng Yu</b>, Eunbyung Park, Alexander C. Berg, Tamara L. Berg<br>
                        </div>
                        <div>
                            [<a target="_blank"  href="papers/iccv15_madlibs.pdf">Paper</a>][<a target="_blank" href="http://tamaraberg.com/visualmadlibs/">Project</a>][<a target="_blank" href="images/iccv15_madlibs.mp4">Spotlight Video</a>][<a target="_blank" href="papers/iccv15_madlibs_supp.pdf">Supplementary File</a>]
                        </div>
                    </div>
                    </td>
                </tr>

                <!-- Dictionary learning -->
                <tr>
                    <td id="Dictionary learning" class="thumb-column">
                        <img class="project-thumb" src="images/aaai15_dictionary.png">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Dictionary Learning with Mutually Reinforcing Group-Graph Structures</b>
                        </div>
                        <a target="_blank" href="http://www.aaai.org/Conferences/AAAI/aaai15.php">AAAI 2015</a>
                        <div class="description">
                            <b>Licheng Yu</b>*, Hongteng Xu*, Hongyuan Zha, Yi Xu<br> (* denotes equal contribution)
                        </div>
                        <div>
                            [<a target="_blank" href="papers/aaai15_dl.pdf">Paper</a>]
                        </div>
                    </div>
                    </td>
                </tr>

                <!-- vector sparse representation -->
                <tr>
                    <td id="Quaternion sparse representation" class="thumb-column">
                        <img class="project-thumb" src="images/icme13_kqsvd3.jpeg">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Vector Sparse Representation of Color Image Using Quaternion Matrix Analysis</b>
                        </div>
                        IEEE Transactions on Image Processing, <a target="_blank" href="http://www.signalprocessingsociety.org/publications/periodicals/image-processing/">TIP 2015</a>
                        <div class="description">
                            Yi Xu, <b>Licheng Yu</b>, Hongteng Xu, Truong Nguyen, Hao Zhang<br>
                        </div>
                        <div>
                            [<a target="_blank" href="papers/tip15_kqsvd.pdf">Paper</a>][<a target="_blank" href="code/Quaternion%20Dictionary%20Learning.zip">Code</a>]
                        </div>
                    </div>
                    </td>
                </tr>

                <!-- quaternion sparse representation -->
                <tr>
                    <td id="Quaternion sparse representation" class="thumb-column">
                        <img class="project-thumb" src="images/tip15_dictionary.jpeg">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Quaternion-based Sparse Representation of Color Image</b>
                        </div>
                        IEEE International Conference on Multimedia and Expo, <a target="_blank" href="http://www.icme2013.org/">ICME 2013</a>
                        <div class="description">
                            <b>Licheng Yu</b>, Yi Xu, Hongteng Xu, Hao Zhang<br>
                        </div>
                        <div>
                                [<a target="_blank" href="papers/icme13_kqsvd.pdf">Paper</a>][<a target="_blank" href="papers/icme13_kqsvd_supp.pdf">Supplementary File</a>]<font color="red"> (Oral presentation)</font>
                            </div>
                        </div>
                        </td>
                    </tr>

                    <!-- Image super-resolution (VCIP13) -->
                    <tr>
                    <td id="Super Resolution" class="thumb-column">
                        <img class="project-thumb" src="images/vcip13_SR.jpg">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Single Image Super-resolution via Phase Congruency Analysis</b>
                        </div>
                        IEEE Visual Communications and Image Processing, VCIP 2013
                        <div class="description">
                            <b>Licheng Yu</b>, Yi Xu, Bo Zhang<br>
                        </div>
                        <div>
                            [<a target="_blank" href="papers/vcip13_SR.pdf">Paper</a>]<font color="red"> (Oral presentation)</font>
                        </div>
                    </div>
                    </td>
                </tr>

                <!-- Image super-resolution (ICME13_workshop) -->
                <tr>
                    <td id="Super Resolution" class="thumb-column">
                        <img class="project-thumb" src="images/icme13_workshop.png">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Self-Example Based Super-resolution with Fractal-based Gradient Enhancement</b>
                        </div>
                        IEEE International Conference on Multimedia and Expo, <a target="_blank" href="http://www.icme2013.org/">ICME workshop 2013</a>
                        <div class="description">
                            <b>Licheng Yu</b>, Yi Xu, Hongteng Xu<br>
                        </div>
                        <div>
                            [<a target="_blank" href="papers/icme13_workshop.pdf">Paper</a>]<i><font color="red"></font></i>
                        </div>
                    </div>
                    </td>
                </tr>

                <!-- Image super-resolution (apsipa12) -->
                <tr>
                    <td id="Super Resolution" class="thumb-column">
                        <img class="project-thumb" src="images/apsipa12_result.jpg">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Robust Single Image Super-resolution based on Gradient Enhancement</b>
                        </div>
                        APSIPA Annual Summit and Conference, <a target="_blank" href="http://www.apsipa2012.org/">APSIPA 2012</a>
                        <div class="description">
                            <b>Licheng Yu</b>, Yi Xu, Hongteng Xu, Xiaokang Yang<br>
                        </div>
                        <div>
                            [<a target="_blank" href="papers/apsipa12_SR.pdf">Paper</a>][<a target="_blank"  href="images/sea_contrast_cs1.mp4">Sample Video</a>]
                        </div>
                    </div>
                    </td>
                </tr>
                </table>
            </div>
			<div class="Miscellaneous">
				<h3>Miscellaneous</h3>
				<div class="projects-table">
				<table>
                    <!-- tutorial Talk -->
                    <tr>
                        <td id="Tutorial" class="thumb-column">
                            <img class="project-thumb" src="images/cvpr2020_tutorial.png">
                        </td>
                        <td class="description-column">
                            <div>
                                <div class="project-title">
                                    <b>Self-supervised Learning for Vision-and-Language</b>
                                </div>
                                <a target="_blank" href="https://rohit497.github.io/Recent-Advances-in-Vision-and-Language-Research/">Recent Advances in Vision-and-Language Research</a></br>
                                <a target="_blank" href="http://cvpr2020.thecvf.com/program/tutorials">CVPR 2020 Tutorial</a>
                                <div class="description">
                                    <b>Licheng Yu</b>, Linjie Li, Yen-Chun Chen
                                </div>
                                <div>
                                    [<a target="_blank" href="https://rohit497.github.io/Recent-Advances-in-Vision-and-Language-Research/slides/tutorial-part5-pretraining.pdf">Slides</a>][<a target="_blank" href="https://www.youtube.com/watch?v=C4UQWJcp7w4">Youtube</a>][<a target="_blank" href="https://www.bilibili.com/video/BV1oD4y1D7V2">Bilibili</a>]
                                </div>
                            </div>
                        </td>
                    </tr>
                    <!-- VQA 2020 Winner -->
                    <tr>
                        <td id="vqa 2020 challenge" class="thumb-column">
                            <img class="project-thumb" src="images/vqa2020.jpg">
                        </td>
                        <td class="description-column">
                            <div>
                                <div class="project-title">
                                    <b>Revisiting Grid Features for VQA</b>
                                </div>
                                <div class="description">
                                    Duy-Kien Nguyen, Huaizu Jiang, Vedanuj Goswami, <b>Licheng Yu</b>, Xinlei Chen
                                </div>
                                <a target="_blank" href="https://visualqa.org/roe.html"><font color="red">Winner of</font> VQA 2020 Challenge</a>
                                <div>
                                    [<a target="_blank" href="https://visualqa.org/workshop.html">Workshop</a>][<a target="_blank" href="https://drive.google.com/file/d/1j9QE6xBq7Al_92ylmQEO4Ufq4f5n3Awa/view">Slides</a>][<a target="_blank" href="https://www.youtube.com/watch?v=XvSWxpdbJxo&feature=youtu.be">Video</a>]
                                </div>
                            </div>
                        </td>
                    </tr>
                <!-- Gobang -->
                <tr>
                    <td id="Gobang" class="thumb-column">
                        <img class="project-thumb" src="images/gobang.jpg">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Gobang Android App (AI mode + 2-player mode)</b>
                        </div>
                        <div class="description">
                            <b>Licheng Yu</b><br>
                        </div>
                        <div>
                            [<a target="_blank" href="papers/gobang_report.pdf">Technical Report</a>][<a target="_blank" href="https://youtu.be/l_NZmUsICbo">Demo</a>]
                        </div>
                    </td>
                </tr>
                <!-- Egocentric Vision -->
                <tr>
                    <td id="Egocentric Vision" class="thumb-column">
                        <img class="project-thumb" src="images/gt13_egovision.jpg">
                    </td>
                    <td class="description-column">
                    <div>
                        <div class="project-title">
                            <b>Skill Measurement via Egocentric Vision in Wetlab</b>
                        </div>
                        <div class="description">
                            <b>Licheng Yu</b>, Yin Li, James Rehg<br>
                        </div>
                        <div>
                            [<a target="_blank" href="papers/wetlab.pdf">Technical Report</a>]
                        </div>
                    </div>
                    </td>
                </tr>
				</table>
				</div>
				</div>
			</div>

            <br/>
            <br/>
            <div>
                <b>PhD Thesis:</b>
                "Question Answering, Grounding, and Generation for Vision and Language"
                [<a target="_blank" href="papers/thesis2.pdf">PDF</a>][<a target="_blank" href="https://youtu.be/3l2Gsasqfvg">Talk</a>]
            </div>

		</div>


	</body>
</html>
